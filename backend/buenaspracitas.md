LOs agentes actuales soporaran el nuevo flujo?
Estas son algunas practucas buenas para la creacion de los agentes:
 Los agentes son software, no magia. Muchas aplicaciones basadas en LLM en producción son principalmente software, con el LLM actuando como una función que convierte lenguaje natural en JSON. No se necesita ser un experto en IA para aplicar estos principios; son fundamentalmente ingeniería de software.
• No todos los problemas necesitan un agente. A veces, un script simple o un código determinista es una solución más eficiente y fácil de mantener. Se proporciona el ejemplo de intentar construir un agente DevOps para compilar un proyecto, que al final hubiera sido más sencillo con un script bash.
• La fiabilidad es clave, y ir más allá del 70-80% es difícil. Muchas personas logran construir agentes que funcionan al 70-80%, lo cual puede ser suficiente para entusiasmar, pero para alcanzar una mayor calidad, se enfrentan a la complejidad de la ingeniería inversa de cómo se crean los prompts o cómo se pasan las herramientas, a menudo llevando a desechar el trabajo y empezar de cero.
• Los "12 factores de los agentes de IA" buscan ayudar a construir agentes más confiables y rápidos. Estos son patrones modulares que se pueden aplicar al código existente para mejorar las aplicaciones basadas en LLM, y no son un enfoque "anti-framework", sino una lista de deseos para que los frameworks satisfagan las necesidades de desarrolladores que buscan alta fiabilidad y rapidez.
• Los LLM son funciones puras y el contexto es crucial.
    ◦ Posea sus prompts (Factor 2): Para superar ciertas barreras de calidad, es probable que termine escribiendo cada token de su prompt a mano, optimizando la densidad y claridad de la información que pasa al LLM. La fiabilidad de su agente depende de la calidad de los tokens que el LLM genera, lo cual está determinado por los tokens que usted le proporciona.
    ◦ Posea su construcción de la ventana de contexto: Debe ser deliberado sobre cómo construye su ventana de contexto y qué información incluye. Esto incluye manejar errores de forma inteligente (resumirlos en lugar de poner todo el stack trace) y no poner cosas a ciegas.
• El uso de herramientas es simplemente JSON y código (Factor 4). La idea de que el uso de herramientas es "mágico" o que una "entidad etérea" interactúa con el entorno es una abstracción errónea. Un LLM emite JSON, que luego es procesado por código determinista (como un bucle o una declaración switch).
• Sea dueño de su flujo de control (Factor 8).
    ◦ Aunque la idea de que un LLM determine el siguiente paso de forma autónoma suena atractiva, la implementación ingenua con un bucle simple (LLM determina el siguiente paso, se construye la ventana de contexto, y se repite hasta que el LLM diga que ha terminado) no funciona bien en la práctica, especialmente para flujos de trabajo más largos, debido a las ventanas de contexto.
    ◦ Controle y limite la cantidad de tokens en la ventana de contexto para obtener resultados más estrictos, mejores y más confiables.
    ◦ Un agente es, en esencia: un prompt que da instrucciones para seleccionar el siguiente paso; una declaración switch que toma la salida JSON del modelo y hace algo con ella; una forma de construir la ventana de contexto; y un bucle que determina cuándo, dónde, cómo y por qué se sale. Al ser dueño de su flujo de control, puede implementar lógicas como interrupciones, cambios y resúmenes.
• Gestione su estado (de ejecución y de negocio).
    ◦ Las herramientas ofrecen conceptos como el paso actual y los reintentos, pero también debe gestionar su estado de negocio (mensajes, datos mostrados al usuario, aprobaciones pendientes).
    ◦ Debe poder iniciar, pausar y reanudar el estado de su agente como con cualquier API estándar. Esto implica serializar la ventana de contexto directamente en una base de datos cuando una herramienta de ejecución prolongada interrumpe el flujo, y luego recargar ese estado para reanudarlo.
    ◦ Los agentes deben ser stateless; usted debe ser dueño y gestionar el estado como desee.
• Utilice microagentes (agentes pequeños y enfocados). Las implementaciones exitosas combinan un flujo de trabajo mayoritariamente determinista con bucles de agente muy pequeños (de 3 a 10 pasos) para tareas específicas. El LLM se integra en su código para partes específicas, y con el tiempo, puede asumir tareas más grandes, pero siempre con un diseño que garantice la calidad.
• Contacte a humanos con herramientas. Es crucial integrar la interacción humana en el flujo del agente. Esto permite que el modelo comunique necesidades como "terminado", "necesita aclaración" o "necesita hablar con un gerente", llevando la intención a la primera generación de tokens y permitiendo a los usuarios interactuar con los agentes en plataformas donde ya están (correo electrónico, Slack, Discord, SMS).
• Encuentre la vanguardia y pruebe todo. La clave es descubrir cómo hacer lo que el modelo apenas puede hacer de manera confiable, diseñando la fiabilidad en el sistema. Debe probar todo para encontrar lo que funciona mejor.
• Enfoque el desarrollo en las partes difíciles de la IA. Las herramientas y frameworks deberían encargarse de las otras complejidades del software, permitiéndole dedicar todo su tiempo a los aspectos difíciles de la IA: acertar los prompts, el flujo y los tokens.
• Los agentes técnicos son mejores con las personas: Encuentre formas de permitir que los agentes colaboren con los humanos.